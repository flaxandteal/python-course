{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# The Bear Necessities\n",
    "## Getting to grips with pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "After the previous session, with its introductory plotting and numerical data handling, by far the most common request for this session was data analysis tools. The second request was that we deal with some Excel importing. In Python, the primary analysis tool we can use for general datasets is called **pandas**. To quote the docs (emphasis mine):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[http://pandas.pydata.org/](http://pandas.pydata.org/)\n",
    "\n",
    ">  Python has long been great for **data munging and preparation**,\n",
    ">  but less so for **data analysis and modeling**.\n",
    "\n",
    ">  **pandas** helps **fill this gap**, enabling you to carry out your **entire data analysis workflow in Python** without having to switch to a more domain specific language like R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "My summary is, pandas is to Excel as numpy/scipy/etc. are to MATLAB. You don't have the nice GUI interface, but you do have easily scriptable automation and all the power of Python to do pretty much anything with your data. In this session, we will begin by importing an XLS file and doing some basic manipulation. I'll show you a slightly quicker way, and then we will get on with some analysis. Time permitting, we will do some plotting, and more advanced pandas techniques, before finishing off by dynamically incorporating online database data into our XLS analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "If you are thinking of experimenting with some data, I would highly recommend you check out OpenDataNI:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![OpenDataNI Screenshot](images/screenshot-opendatani.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Good in general, but unfortunately for this session, they force public sector bodies to upload data in standardized, universally-supported formats, so not Microsoft Excel. Instead, we will grab some data from a longer-running project, from when it was common to accept a broader range of formats: [data.gov.uk](https://data.gov.uk/data/search?res_format=XLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# data.gov.uk\n",
    "## Social Trends 2011\n",
    "\n",
    "![data.gov.uk Excel data](images/screenshot-datagovuk.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Conveniently, as this is under the Open Government License, I've been able to put a copy of the relevant Excel file in the *data* folder below this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In the last session, we spent a lot of time using Jupyter as a workflow, stepping from one box to the next. That's fine for experimenting with data, or keeping a journal of what you're doing, but not so useful if you want a repeatable script. This afternoon, we are going to use a standalone IDE to do some scripting. However, for the moment, keeping our notes and our code in one place is handy, so we will work through a series of short scripts in Jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Firstly though, we will need to check that the libraries pandas and xlrd are installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Second, we should take a look at the Excel file and see what we are actually dealing with..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Thirdly, we run the script below. Don't worry about the detail just this second, we will work through it step by step, as there are a few subtleties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# [A]\n",
    "excel_file = pd.ExcelFile('data/expenditure.xls')\n",
    "\n",
    "# [B] Pick out an interesting table - 2011 value of household expenditure by purpose\n",
    "df = excel_file.parse('Table 1')\n",
    "\n",
    "# [C] Cut down to the relevant table\n",
    "df = df.loc[4:26]\n",
    "\n",
    "# [D] Challenge here!\n",
    "#df = df.******(how='***', axis=0)\n",
    "#df = df.******(how='***', axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off, lets have a look at that Excel file we loaded. There are a few Excel-file-specific properties methods of interest, but we will start with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "excel_file.sheet_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The basic unit of pandas operations is the DataFrame, kind of like numpy is centred on numpy arrays. In fact, originally the pandas DataFrame subclassed numpy arrays, (but nowadays it's a completely separate implementation). A DataFrame is a bit like a spreadsheet or database table. In my head they look a bit like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![What I think a data-frame looks like](images/dataframe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Notice that multi-level indexes are fine - we can sub-divide rows even further than that if we really want. So, this really corresponds to a single-table spreadsheet. So we pick our sheet out of the Excel document at line `[B]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "df = excel_file.parse('Table 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "What's this data look like? Well, remember, in Excel it looked like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Screenshot of Excel data](images/screenshot-excel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In pandas it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "You should try this without the `.head()`. Well... all the data is there, but a whole load of `NaN` entries corresponding to the empty cells. The table we actually want is in the (Excel) range [A6:H28]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can grab a row using the syntax `df.loc[ROW]`, where `ROW` is the row we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now, if we grab row 6, what happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default column names - \"Unnamed: 1\", etc. - aren't very helpful. We can use a handy pandas trick to rename column headings to letters of the alphabet..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "column_count = len(df.columns)\n",
    "df.columns = list(string.ascii_uppercase)[0:column_count]\n",
    "\n",
    "# Show our sheet again, with new column names\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This actually accepts the list slicing syntax we learned last time, e.g. `df.loc[6:7]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[6:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Notice that when we picked out a single row we got something that looks a bit different - an ASCII table instead of one of those nice formatted HTML-y things. That's because pandas has a concept called Series, like a 1D array, with rows. When we pick a single row or column, that's what we get back. Don't worry too much about why they are different, just think of them as a single-dimensional list of spreadsheet cells, but bear in mind, 1D data isn't actually a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "If you're really astute, you will have noticed that `df.loc[6]` is not the same as Excel's [A6:H6]. Quiz question - why?\n",
    "<span style='color: white; background: white'>(a) Python is zero-indexed and (b) it treats the first row in Excel as the header, so in total Python is different by two</span> <-- (the answer is hidden here if you're desperate, highlight the text with your mouse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So actually, the header row in Excel, [A6:H6], corresponds to `df.loc[4]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "At step `[C]` we trim down a bit and extract only table we are interested in. Notice that we don't worry about the columns - pandas has spotted no more data comes after column H and stops adding them in at that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df = df.loc[4:26]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img alt=\"Cut down screenshot showing the DataFrame's new coverage\" src=\"images/screenshot-excel-cutdown.png\" style='max-height: 400px' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[4:6, \"B\":\"C\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "As a side-point, you can slice in two directions, as with numpy arrays. For instance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Challenge\n",
    "## Fix the stars in `[D]`\n",
    "\n",
    "Remember to vote when finished! https://www.strawpoll.me/20175488\n",
    "\n",
    "You'll need the pandas docs [http://pandas.pydata.org/pandas-docs/stable/api.html](http://pandas.pydata.org/pandas-docs/stable/api.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "So, now we just need to deal with all those `NaN`s. Thankfully, this is something pandas is especially good at. Generally speaking, you can just work with the data and it will ignore those `NaN` entries when you sum, or average, or whatever. However, this table only has `NaN`s in full rows and columns, so (except for one row) we can excise them very neatly. However, this is a challenge for you - fill in the 9 mising characters in `[D]`, marked as asterisks, to complete the script above! You should get printed output showing many rows and columns, all with string or numeric entries. There will a few `NaN`s left though in a row..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The final few `NaN`s are actually to do with some cells having spaces or so forth in them. Because we used the `all` argument, all cells in the row or column must be `NaN` for it to be removed. But this is really a loading-time problem - how do we tell pandas to recognise a space character as a NaN value (also known as NA values)? Start here... [http://pandas.pydata.org/pandas-docs/stable/generated/pandas.ExcelFile.parse.html#pandas.ExcelFile.parse](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.ExcelFile.parse.html#pandas.ExcelFile.parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Challenge\n",
    "\n",
    "Can you modify the ExcelFile class's `parse(...)` arguments to treat cells with only a space in them as `NaN`s?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Don't read through the following code snippets until you work it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Doing something with the data\n",
    "## Making it all worthwhile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "## SECOND VERSION OF SCRIPT\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# [A]\n",
    "excel_file = pd.ExcelFile('data/expenditure.xls')\n",
    "\n",
    "# [B] Pick out an interesting table - 2011 value of household expenditure by purpose\n",
    "df = excel_file.parse('Table 1', na_values=' ')\n",
    "\n",
    "# [C] Cut down to the relevant table\n",
    "df = df.loc[4:26]\n",
    "\n",
    "# [D] Strip out empty rows and columns\n",
    "df = df.dropna(how='all', axis=0)\n",
    "df = df.dropna(how='all', axis=1)\n",
    "\n",
    "# [E] Reindex/column to desired table\n",
    "df.loc[4, \"Table 1\"] = 0               # E1\n",
    "df.columns = df.loc[4, :].astype(int)  # E2\n",
    "df = df[1:]                            # E3\n",
    "df.index = df[0]                       # E4\n",
    "df.index.name = \"Category\"             # E5\n",
    "df = df.loc[:, \"A\":]                   # E6\n",
    "df.columns.name = \"Years\"              # E7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise: E-numbers\n",
    "\n",
    "Have a look at Etherpad, and get the number of your introductory row - if it's over 7, subtract 7. Have a look at the corresponding \"En\" row above and work out exactly what it does and why. You may need to think through some of the lines above.\n",
    "\n",
    "In Etherpad, where there is a space left for notes on this challenge put as full and descriptive notes as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## THIRD VERSION OF SCRIPT\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Pick out an interesting table - 2011 value of household expenditure by purpose\n",
    "df = pd.read_excel(\n",
    "    'data/expenditure.xls',\n",
    "    sheet_name='Table 1',\n",
    "    na_values=' ',\n",
    "    header=5,\n",
    "    index_col=0,\n",
    "    usecols=(0, 1, 2, 3, 4, 5, 7)\n",
    ").dropna()\n",
    "\n",
    "df.index.name = \"Category\"\n",
    "df.columns.name = \"Years\"\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now, there are a couple of take-away points here. First, I'm really irritating. Second, in Python, parsing functions also contain bonus arguments to do standard transformations, whether it's from CSV, Excel, SQL or whatever - always have a quick check through the documentation to see if you are re-inventing the wheel.\n",
    "\n",
    "However, when you need more control - say, you have to play around with messier data than this, or some of those arguments (like desired columns) must be calculated *after* the data is loaded - then the first examples show how you can tweak pandas data to do exactly what you want.\n",
    "\n",
    "A formatting point: while Python determines code blocks entirely by indenting, between parens (and many other places), whitespace does not matter, and so I can spread the parse call over several lines. As `read_excel` returns DataFrame, we can call `dropna()` on the returned value directly and get back the tidied DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "If we look back at the Excel file, we see that the 2009.1 column is actually a different unit. The first five columns are percentages comparative based on 1971 - so, from 1971 to 2009, the amount spent on food and drink has risen by about 50% (to 151%). In case you're thinking, \"but bread cost a tenth of today's price in 1970!\", bear in mind that this is already adjusted for inflation.\n",
    "\n",
    "The last column is an absolute value in billions sterling (don't ask me why \"current prices\" are marked 2009 in a spreadsheet ONS marked 2011). So maybe a subindex with \"as % of 1971\" and \"in today's currency\" as a breakdown would be handy, rather than mixing up the types in a single row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "## FOURTH VERSION OF SCRIPT\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Pick out an interesting table - 2011 value of household expenditure by purpose\n",
    "df = pd.read_excel(\n",
    "    'data/expenditure.xls',\n",
    "    sheet_name='Table 1',\n",
    "    na_values=' ',\n",
    "    header=5,\n",
    "    index_col=0,\n",
    "    usecols=(0, 1, 2, 3, 4, 5, 7)\n",
    ").dropna()\n",
    "\n",
    "df.index.name = \"Category\"\n",
    "df.columns.name = \"Years\"\n",
    "\n",
    "# [F]\n",
    "df.columns = map(str, df.columns)\n",
    "perc1971 = df.loc[:, '1971': '2009']\n",
    "todaysCurrency = pd.DataFrame({'2009': df['2009.1']})\n",
    "\n",
    "# [G]\n",
    "joined_df = pd.concat((perc1971, todaysCurrency), axis=1, keys=(\"%1971\", \"today's money\"))\n",
    "\n",
    "# [H]\n",
    "stacked_df = joined_df.stack(level=0)\n",
    "\n",
    "# [I]\n",
    "def calculate_prices_in_todays_money_for_row(x):\n",
    "    if x.name[1] != \"today's money\":\n",
    "        return x\n",
    "    \n",
    "    percent_row = (x.name[0], \"%1971\")\n",
    "    ratio = stacked_df.loc[percent_row, :]\n",
    "    ratio2009 = stacked_df.loc[percent_row, '2009']\n",
    "    \n",
    "    return ratio * x['2009'] / ratio2009\n",
    "\n",
    "filled_df = stacked_df.*****(calculate_prices_in_todays_money_for_row, ****=*)\n",
    "\n",
    "# [J]\n",
    "# You only need this line in Jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "transposed_df = filled_df.transpose()\n",
    "transposed_df[\"Communication\"].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now, this is a bit more intricate. What we do use pandas understanding of multiple levels of columns. I say 'understanding' because pandas actually knows what to do with sub-columns, what they are, how they are layered and how to manipulate them, as opposed to a basic Excel spreadsheet that just displays what you put in the cells.\n",
    "\n",
    "So, we want to stop mixing up our several ratio columns and our actual currency column..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Make the cols strings, not integers\n",
    "df.columns = map(str, df.columns)\n",
    "# Pick out all rows and cols 1971 to 2009\n",
    "# (remember, loc lets us identify them by actual row/column names)\n",
    "perc1971 = df.loc[:, '1971': '2009'] # [F]\n",
    "# Pick out the 2009 column, and make it a DataFrame of its own\n",
    "# We do this with a 1-element dictionary (remember those?)\n",
    "todaysCurrency = pd.DataFrame({'2009': df['2009.1']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "perc1971.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "todaysCurrency.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "That's not a whole lot of use, unless we can put them back together..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "joined_df = pd.concat((perc1971, todaysCurrency), axis=1, keys=(\"%1971\", \"today's money\")) # [G]\n",
    "joined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Essentially what we did was give pandas a tuple with the DataFrames we wanted to tied together, and matching keys to group them under (lists instead of tuples would have been fine). We could also have just appended the columns, but we want to distinguish between these two sets of columns, so two-level indexing makes sense. We also passed `axis=1` to indicate we wanted this done to the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now that's starting to look nicer than ONS published it. There's a bit more touching up of categories we could do (\"totals\" for a start seem to be in the wrong place, and footnote numbers have crept in), but that's an exercise for the overly keen reader.\n",
    "\n",
    "Still... it feels like there's something missing... we have inflation-adjusted ratios of spending vs 1970 (the %1970 columns), but only an actual currency value for 2009. Surely we can expand this to match? Presumably, if £83bn is 151% of 1971 spending (look at 2009 Food column) then people in 1971 spent the equivalent of around £50bn in today's money. That's kind of interesting, if you're into that sort of thing.\n",
    "\n",
    "Now, given that you're all doing PhDs and you're planning to apply these techniques to your data, you'll probably end up in a scenario where you are sitting with a load of empty datacells going, \"hey, I know an interesting way to fill these!\" and it will be approximately as interesting to anybody else as \"I wonder what the inflation adjusted household expenditure on furnishings was in 1971?\" sounds to you right now. So it's a perfect example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "First off - we want to use a tool called \"stacking\". This pivots, turning a level of columns into sub-rows. If that doesn't make much sense to you right now, I'll show you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "joined_df.stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "So, structurally, we now have what we want - the level argument told pandas which level (the entry-type or year) to swap into the rows. However, we still want the years as columns, we just want to be able to see \"%1970\" or \"today's money\" for each expenditure category..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "stacked_df = joined_df.stack(level=0) # [H]\n",
    "stacked_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "That's better. That's structurally what we want. Now we have the task of filling in the values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Well, we will need a function like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# [I]\n",
    "def calculate_prices_in_todays_money_for_row(x):\n",
    "    if x.name[1] != \"today's money\":\n",
    "        return x\n",
    "    # ... some stuff here ...\n",
    "    return row_with_the_NaNs_filled_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We can then apply this to every row `x`. Note that, when we have a multi-level index, the `x.name` value will be a tuple, with the top-level index as first entry and sub-index as second. That means that this function will simply send back the unmodified row, *unless* it is one of the \"today's money\" ones. So what do we do to fill in the missing values in the row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For each year $y$:\n",
    "\n",
    "$$ \\frac{\\unicode{163}_y}{\\%_y} = \\frac{\\unicode{163}_{2009}}{\\%_{2009}}$$\n",
    "\n",
    "or, equivalently,\n",
    "\n",
    "$$ \\unicode{163}_y = \\unicode{163}_{2009} \\times \\frac{\\%_y}{\\%_{2009}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Why? Well think about the first example with 1971:\n",
    "\n",
    "$$ \\unicode{163}_{1971} = \\unicode{163}83\\mathrm{bn} \\times \\frac{100\\%}{151\\%} = \\unicode{163}55\\mathrm{bn}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Pandas is nice, in that we can forget about looping over each year in the row, and just do the sum with the entire row - pandas knows how to add, subtract and multiply them element-wise. If you're still with me so far, great - if not, the take-away is that this could be any complex formula or data-filling trick or otherwise that you need to apply to a subset of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# [I]\n",
    "def calculate_prices_in_todays_money_for_row(x):\n",
    "    if x.name[1] != \"today's money\":\n",
    "        return x\n",
    "    \n",
    "    # Get the name of the row with all the percentages\n",
    "    # for this category\n",
    "    percent_row = (x.name[0], \"%1971\")\n",
    "    # Get the values for the percentage ratios\n",
    "    ratio = stacked_df.loc[percent_row, :]\n",
    "    # Get the 2009 one (e.g. for Food this is 151%)\n",
    "    ratio2009 = stacked_df.loc[percent_row, '2009']\n",
    "    \n",
    "    # Do the sum on the last slide\n",
    "    return ratio * x['2009'] / ratio2009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Here we use the `loc` method, instead of the `iloc` method - this specifically tells pandas we are slicing by **label**, not by **position**. That is, we want the column *called* 2009, not the 2009th column. Like I said, the formula in the return statement is operating on Series (which are like lists or arrays), not scalars, but as long as they are same length Series (`ratio` and `x[2009]`) or are scalars (`ratio2009` is a float), pandas knows to do everything element-wise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "OK, grand. So, what I'm going to do now is let you work out how to actually apply this to the DataFrame. If you're struggling with what is going on, feel free to skip ahead to grab the answer and work back or ask questions about what's just happened - if you feel fairly happy, then this your next exercise. Either way..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Put up your stars\n",
    "## Once you are content, put up arrows\n",
    "Or I will be sitting here a very long time.\n",
    "\n",
    "Either\n",
    "\n",
    "1. revise the last few steps (F, G, H, I) or\n",
    "2. try and fill the blanks in the line between `[I]` and `[J]` to apply the filling function to the data.\n",
    "\n",
    "If you find it, put the relevant link to the docs in Etherpad for others!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "So if you got that to run, then you should see a nice plot. If not, copy the relevant line from the script below to the one above and run it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now, the following should have no missing values..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "filled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Last comment on that script - pandas makes basic plotting very simple - we can just call `plot(...)` on our dataset, and using the `kind` argument can get the value shown in several ways. We do a transpose at `[J]` to get the categories along the plot's `x` axis, and note that pandas understands the concept of sub-indexes, grouping the bars automatically. But this really isn't very useful - currency and ratio are completely different scales!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Plot Thickens\n",
    "## Like graphical soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "## FIFTH VERSION OF SCRIPT\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pick out an interesting table - 2011 value of household expenditure by purpose\n",
    "df = pd.read_excel(\n",
    "    'data/expenditure.xls',\n",
    "    sheet_name='Table 1',\n",
    "    na_values=' ',\n",
    "    header=5,\n",
    "    index_col=0,\n",
    "    usecols=(0, 1, 2, 3, 4, 5, 7)\n",
    ").dropna()\n",
    "\n",
    "# Uncommented for presenting only!! Commented version at end of presentation\n",
    "df.index.name = \"Category\"\n",
    "df.columns = map(str, df.columns)\n",
    "df.columns.name = \"Years\"\n",
    "perc1971 = df.loc[:, '1971': '2009']\n",
    "todaysCurrency = pd.DataFrame({'2009': df['2009.1']})\n",
    "df = pd.concat((perc1971, todaysCurrency), axis=1, keys=(\"%1971\", \"today's money\"))\n",
    "df = df.stack(level=0)\n",
    "\n",
    "def calculate_prices_in_todays_money_for_row(x):\n",
    "    if x.name[1] != 'today':\n",
    "        return x\n",
    "    \n",
    "    percent_row = (x.name[0], '%1971')\n",
    "    ratio = df.loc[percent_row, :]\n",
    "    ratio2009 = df.loc[percent_row, '2009']\n",
    "    \n",
    "    return ratio * x['2009'] / ratio2009\n",
    "\n",
    "df = df.apply(calculate_prices_in_todays_money_for_row, axis=1)\n",
    "\n",
    "# You only need this line in Jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "# [K]\n",
    "fig = plt.figure()\n",
    "\n",
    "# Create a set of axes\n",
    "ax_left = fig.add_subplot(111)\n",
    "# And another sharing an x-axis\n",
    "ax_right = ax_left.twinx()\n",
    "\n",
    "# Grab a specific category, just to be clearer\n",
    "category = df.transpose()[\"Communication\"]\n",
    "\n",
    "# Show bars for both the 1971% ratio (left y-axis) and today's money (right y-axis)\n",
    "category[\"%1971\"].plot(kind='bar', ax=ax_left)\n",
    "category[\"today's money\"].plot(kind='bar', ax=ax_right, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now this script is getting a bit longer - don't worry, there is a tidier version at the end (not massively tidied, just the kind of quick tidy that it is totally reasonable to expect you to do when you're doing PhD coding!) Anyhow, skip down to the plotting part `[K]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "# Create a set of axes\n",
    "ax_left = fig.add_subplot(111)\n",
    "# And another sharing an x-axis\n",
    "ax_right = ax_left.twinx()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This trick here allows you to add two scales to a graph - one on the right, one on the left. Now pandas can show both bars (overlapping) and scaled correctly! Try running the script..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Something strange is going on here - we only see one set of bars. In fact, this is correct, because the currency is simply a scaling of the %1971 column - that's how we calculated it - then when we scale the bars we made to the same overall size on the plot, shock and horror, they match. This isn't a maths lesson, so if this confuses you or is unexpected, that isn't core to what's happening on a programming level, rather the kind of thing you might come across in your data analysis, dealing with interpolated or filled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In any case, plotting two sets of identical bars on different scales is clearly pretty useless. What would be more interesting would be, rather than looking at inflation-adjusted ratio and inflation-adjusted pound sterling price, how about inflation-adjusted ratio and actual price payed over the counter in 1971?\n",
    "\n",
    "While I wait for the clamour to die down and you all to take your seats again (make sure you get the party poppers off the lights first), we'll start thinking of a way to do that..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# To infinity and beyond\n",
    "## Integrate with other sources\n",
    "\n",
    "For the next bit, we will need to install **Quandl** via pip3.\n",
    "\n",
    "Have a quick look here [https://www.quandl.com/collections/uk/uk-inflation](https://www.quandl.com/collections/uk/uk-inflation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Quandl is an online service that provides Python bindings to grab its online data (using JSON, as is standard nowadays). Note that, you can make only 50 calls a day, so an extra line or two to save a dump when you first use it (and only call again if the dump is missing) will give you extra mileage. However, we are going to cheat and just call it once manually..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import quandl\n",
    "cpi = quandl.get(\"UIFS/CPI_GBR\", returns=\"pandas\")\n",
    "cpi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "So what might we want to do with that? Well, the consumer price index (CPI) gives us a way to adjust for inflation. Basically, it says £4.42 in 1948 would buy you the same amount of basic goods as £111.28 in 2009. So we can use this to turn our inflation-adjusted values back into non-inflation-adjusted values - we divide by the CPI. To make it a bit clearer, if we say £55bn was spent on Food in 1971, *in today's money* then we can say (approximately) $$ \\unicode{163}55\\mathrm{bn} \\times \\frac{CPI_{1971}}{CPI_{today}}$$ was spent in 1971-money. Ideally, if you looked up some government record from 1971, the first year of decimal UK currency, and it said \"Ye Olde Expenditure for This Yeare of the HouseHoldes on the Foods and Stuffe of Similar Likeness\", then that should be about the number it says. Please note, 1971 isn't that long ago - I'm just about in that generation, so it's not allowed to be.\n",
    "\n",
    "Again, if you find this mysterious, you can just use the formula like any other mysterious statistical formula - this isn't a maths course, we just need it for sensibly coding the analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "## SIXTH VERSION OF SCRIPT\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pick out an interesting table - 2011 value of household expenditure by purpose\n",
    "df = pd.read_excel(\n",
    "    'data/expenditure.xls',\n",
    "    sheet_name='Table 1',\n",
    "    na_values=' ',\n",
    "    header=5,\n",
    "    index_col=0,\n",
    "    usecols=(0, 1, 2, 3, 4, 5, 7)\n",
    ").dropna()\n",
    "\n",
    "df.index.name = \"Category\"\n",
    "df.columns = map(str, df.columns)\n",
    "df.columns.name = \"Years\"\n",
    "\n",
    "perc1971 = df.loc[:, '1971': '2009']\n",
    "todaysCurrency = pd.DataFrame({'2009': df['2009.1']})\n",
    "\n",
    "df = pd.concat((perc1971, todaysCurrency), axis=1, keys=(\"%1971\", \"currency\"))\n",
    "stacked_df = df.stack(level=0)\n",
    "\n",
    "# ======= Pretty much the same down to here, only some label changes ======\n",
    "\n",
    "# Switch around so the rows are years\n",
    "df = stacked_df.transpose()\n",
    "\n",
    "# [L]\n",
    "# Turn the rows into actual DateTimeIndex objects\n",
    "df.index = pd.to_datetime(df.index, format=\"%Y\")\n",
    "\n",
    "# [M]\n",
    "# Grab the CPI values from the online database (if you ran this once, you shouldn't need to uncomment it)\n",
    "#cpi = Quandl.get(\"UIFS/CPI_GBR\", returns=\"pandas\")\n",
    "cpi_series = cpi[\"CPI: ALL ITEMS (INDEX NUMBER)\"]\n",
    "\n",
    "# [N]\n",
    "# Create a Series where all the rows are...\n",
    "our_cpi_values = pd.Series(index=df.index)\n",
    "# Create a Series where the indices are... and the rows are...\n",
    "our_cpi_values = pd.concat((cpi_series, our_cpi_values)).sort_index()\n",
    "# After this none of the rows are...\n",
    "our_cpi_values = our_cpi_values.interpolate()\n",
    "# This command...\n",
    "our_cpi_values = our_cpi_values.reindex(df.index)\n",
    "\n",
    "# [O]\n",
    "date2009 = pd.to_datetime(\"2009-01-01 00:00:00\")\n",
    "def calculate_prices_in_old_money_for_col(x):\n",
    "    if x.name[1] != 'currency':\n",
    "        return x\n",
    "    \n",
    "    percent_col = (x.name[0], '%1971')\n",
    "    \n",
    "    ratio = df.loc[:, percent_col]\n",
    "    ratio2009 = df.loc[date2009, percent_col]\n",
    "    \n",
    "    cpi = our_cpi_values\n",
    "    cpi2009 = our_cpi_values[date2009]\n",
    "    return ratio * x[date2009] * cpi / (ratio2009 * cpi2009)\n",
    "\n",
    "df = df.apply(calculate_prices_in_old_money_for_col)\n",
    "\n",
    "# You only need this line in Jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "# Some fancy plotting\n",
    "fig = plt.figure()\n",
    "ax_left = fig.add_subplot(111)\n",
    "ax_right = ax_left.twinx()\n",
    "\n",
    "# Pick the category we want\n",
    "category = df[\"Communication\"]\n",
    "category[\"%1971\"].plot(kind='bar', ax=ax_left, label=\"% of 1971\")\n",
    "category[\"currency\"].plot(kind='bar', ax=ax_right, color='red', label=\"Currency\")\n",
    "\n",
    "# We do a little more decoration this time\n",
    "plt.title(\"Household Expenditure on Communication\")\n",
    "ax_left.legend(loc=2)\n",
    "ax_left.set_ylabel(\"% of 1971 (inflation-adjusted)\")\n",
    "ax_right.legend(loc=9)\n",
    "ax_right.set_ylabel(\"£ bill. (direct comparison)\")\n",
    "\n",
    "# Update the labels in a pretty way\n",
    "labels = df.index.map(lambda t: t.strftime(\"%Y\"))\n",
    "ax_left.set_xticklabels(labels)  # If you want to know why, comment out this line!\n",
    "ax_left.set_xlabel(\"Year\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The point of this exercise, really, is to show you how you can combine multiple incompatible sources of data in constructive ways that would be extremely hard in other tools. This is just one example - you could be doing this with data from a live feed, or a daily HDF5 astronomical data dump, or a molecular database, anything Python can handle, and bring it into pandas to integrate it with your Excel table.\n",
    "\n",
    "In this particular case, we want to match up the years in the Expenditure data with the timestamps in the CDI data. While you could do this all with the years in columns, to my mind it is neater as the row index, so we start with our stacked dataset and transpose it (run the script above to set up `stacked_df` first):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df = stacked_df.transpose()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now we can get a year, something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[\"1971\"] # instead of stacked_df[1971]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We are going to learn how to interpolate in time - this has many useful applications and is one of pandas' main strengths. Normally, we would be resampling, say, a broken up series of timestamped measurements to try to estimate some value at daily, hourly, monthly or other regular intervals. Here though, we have one series of timestamps and want to match them to another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The first step is to make sure pandas knows our Years in the Expenditure table are actually dates, not some random integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# [L]\n",
    "# Turn the rows into actual DateTimeIndex objects\n",
    "df.index = pd.to_datetime(df.index, format=\"%Y\")\n",
    "df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In one not-so-fell swoop, we convert all of the entries in the index (i.e. row headings, i.e. years) into DateTime objects, and set the index (i.e. the row headings) to the converted values. The pandas output isn't very pretty but it does confirm that. Now what can we do with it?\n",
    "\n",
    "Well, put it with the CPI info..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# [M]\n",
    "cpi[\"CPI: ALL ITEMS (INDEX NUMBER)\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "To mix this data together, we need to be able to map dates we have in the DataFrame index with interpolated values from this series... so begins our next challenge!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Quiz\n",
    "## What on Earth are we doing in Section `[N]`?\n",
    "\n",
    "Stars up! Fill in missing text for each of the ...s in the section [N] comments above. Update Etherpad for reference! The more detail, the better, and if provide links to the docs for each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create a Series where all the rows are...\n",
    "our_cpi_values = pd.Series(index=df.index)\n",
    "# Create a Series where the indices are... and the rows are...\n",
    "our_cpi_values = pd.concat((cpi_series, our_cpi_values)).sort_index()\n",
    "# After this none of the rows are...\n",
    "our_cpi_values = our_cpi_values.interpolate()\n",
    "# This command...\n",
    "our_cpi_values = our_cpi_values.reindex(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Note that I'm being inefficient and creating new objects every time - if you are dealing with large data and want to reuse your data structures, many pandas operations allow you to pass an argument like `overwrite=True` or `copy=False` to force that behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# [O]\n",
    "date2009 = pd.to_datetime(\"2009-01-01 00:00:00\")\n",
    "def calculate_prices_in_old_money_for_col(x):\n",
    "    if x.name[1] != 'currency':\n",
    "        return x\n",
    "    \n",
    "    percent_col = (x.name[0], '%1971')\n",
    "    \n",
    "    ratio = df.loc[:, percent_col]\n",
    "    ratio2009 = df.loc[date2009, percent_col]\n",
    "    \n",
    "    cpi = our_cpi_values\n",
    "    cpi2009 = our_cpi_values[date2009]\n",
    "    return ratio * x[date2009] * cpi / (ratio2009 * cpi2009)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This is really just our old function, slightly adjusted to deal with columns and with the `our_cpi_values` variable used, as I described above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\unicode{163}_{y} = \\unicode{163}_{2009} \\times \\frac{\\%_y}{\\%_{2009}} \\times \\frac{\\mathrm{CPI}_y}{\\mathrm{CPI}_{2009}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This is as complicated as I let this function get. We scale the total figure for 2009 first by the percentage value, then by the CPI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The rest is broadly the same, we just adjust one or two calls for the fact we now have Years in rows, instead of columns. The last bit is plotting - this one is a little nicer... in particular:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# We do a little more decoration this time\n",
    "plt.title(\"Household Expenditure on Communication\")\n",
    "ax_left.legend(loc=2)\n",
    "ax_left.set_ylabel(\"% of 1971 (inflation-adjusted)\")\n",
    "ax_right.legend(loc=9)\n",
    "ax_right.set_ylabel(\"£ bill. (direct comparison)\")\n",
    "\n",
    "# Update the labels in a pretty way\n",
    "labels = df.index.map(lambda t: t.strftime(\"%Y\"))\n",
    "ax_left.set_xticklabels(labels)  # If you want to know why, comment out this line!\n",
    "ax_left.set_xlabel(\"Year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Here we add some axis labels and so forth. The legend locations are a bit strange - numbers 0 to 10, indicating the preferred location - see the [matplotlib.pyplot.legend](http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.legend) docs for the full list of locations.\n",
    "\n",
    "Now, what do we do with the labels? First we take the index, which is a Series (1D array) then we use a member called `map`. This applies a function to each element and returns the mutated list. In this case, our function is a lambda function that takes one argument (a DataTimeIndex) and formats it, just printing the Year component as a string. This is equivalent to:\n",
    "\n",
    "```python\n",
    "    def stringify_date(date):\n",
    "        return date.format(\"%Y\")\n",
    "    \n",
    "    labels = df.index.map(stringify_date)\n",
    "```\n",
    "\n",
    "The lambda function just makes it a bit more neat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "If you run the full script above, you should now see a nice comparison of what people spent on Communications (we got bored with Food and switched) in 1971, in their money, (red) against the inflation-adjusted ratio (blue)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Coup de Grace\n",
    "## Publishable paper to take home"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "One last trick, just to round it off and give us a useful analysis that we could pop into a journal and sit back. Lets produce a series of plots in a row, just like that one, for interesting categories - say, \"Communications\", \"Food\" and \"Alcohol &amp; Tobacco\". How have they all changed (a) in relative terms, compared to say, the price of bread/basic goods, and (b) in absolute terms, the number of 1971, 1981, 1991, etc. pounds sterling that changed hands in those years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "## SEVENTH VERSION OF SCRIPT\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pick out an interesting table - 2011 value of household expenditure by purpose\n",
    "df = pd.read_excel(\n",
    "    'data/expenditure.xls',\n",
    "    sheet_name='Table 1',\n",
    "    na_values=' ',\n",
    "    header=5,\n",
    "    index_col=0,\n",
    "    usecols=(0, 1, 2, 3, 4, 5, 7)\n",
    ").dropna()\n",
    "\n",
    "df.index.name = \"Category\"\n",
    "df.columns = map(str, df.columns)\n",
    "df.columns.name = \"Years\"\n",
    "\n",
    "perc1971 = df.loc[:, '1971': '2009']\n",
    "todaysCurrency = pd.DataFrame({'2009': df['2009.1']})\n",
    "\n",
    "df = pd.concat((perc1971, todaysCurrency), axis=1, keys=(\"%1971\", \"currency\"))\n",
    "df = df.stack(level=0)\n",
    "\n",
    "# Note we have swapped around here!\n",
    "df = df.transpose()\n",
    "\n",
    "# [L]\n",
    "df.index = pd.to_datetime(df.index, format=\"%Y\")\n",
    "\n",
    "# [M]\n",
    "#cpi = quandl.get(\"UIFS/CPI_GBR\", returns=\"pandas\")\n",
    "cpi_series = cpi[\"CPI: ALL ITEMS (INDEX NUMBER)\"]\n",
    "\n",
    "# [N]\n",
    "our_cpi_values = pd.Series(index=df.index)\n",
    "our_cpi_values = pd.concat((cpi_series, our_cpi_values)).sort_index()\n",
    "our_cpi_values = our_cpi_values.interpolate()\n",
    "our_cpi_values = our_cpi_values.reindex(df.index)\n",
    "\n",
    "# [O]\n",
    "date2009 = pd.to_datetime(\"2009-01-01 00:00:00\")\n",
    "def calculate_prices_in_old_money_for_col(x):\n",
    "    if x.name[1] != 'currency':\n",
    "        return x\n",
    "    \n",
    "    percent_col = (x.name[0], '%1971')\n",
    "    \n",
    "    ratio = df.loc[:, percent_col]\n",
    "    ratio2009 = df.loc[date2009, percent_col]\n",
    "    \n",
    "    cpi = our_cpi_values\n",
    "    cpi2009 = our_cpi_values[date2009]\n",
    "    return ratio * x[date2009] * cpi / (ratio2009 * cpi2009)\n",
    "\n",
    "df = df.apply(calculate_prices_in_old_money_for_col)\n",
    "\n",
    "# You only need this line in Jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "categories_of_interest = (\n",
    "    \"Communication\",\n",
    "    \"Food and non-alcoholic beverages\",\n",
    "    \"Alcoholic beverages and tobacco\"\n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(20, 5))\n",
    "\n",
    "for i, category_name in enumerate(categories_of_interest):\n",
    "    ax_left = fig.add_subplot(1, len(categories_of_interest), i + 1)\n",
    "    ax_right = ax_left.twinx()\n",
    "\n",
    "    category = df[category_name]\n",
    "    category[\"%1971\"].plot(kind='bar', ax=ax_left, color='blue', label=\"% of 1971\")\n",
    "    category[\"currency\"].plot(kind='bar', ax=ax_right, color='red', label=\"Currency\")\n",
    "\n",
    "    plt.title(\"Household Expenditure on \" + category_name)\n",
    "    ax_left.legend(loc=2)\n",
    "    ax_left.set_ylabel(\"% of 1971 (inflation-adjusted)\")\n",
    "    ax_right.legend(loc=9)\n",
    "    ax_right.set_ylabel(\"£ bill. (direct comparison)\")\n",
    "\n",
    "    ax_left.set_xticklabels(df.index.map(lambda t: t.strftime(\"%Y\")))\n",
    "    ax_left.set_xlabel(\"Year\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Nothing significant has changed, right down to the plotting section. At that point, we make sure our figure is a sensible shape for several plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "then we loop through our chosen categories, numbering them on the way, and identifying the actual location in the row of plots with `add_subplot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for i, category_name in enumerate(categories_of_interest):\n",
    "    ax_left = fig.add_subplot(1, len(categories_of_interest), i + 1)\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "we drop in the category names in a couple of places, and that is it! The wonderful thing about this approach to analysis is extending functionality as we just did, is often very quick and takes minimal changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "So looking at the final output, what do we see? In all cases, expenditure rises in absolute terms. The UK spends more on everything in absolute terms, year on year. However, with Communication, we see it is growing exponentially, with Alcohol &amp; Tobacco it is logarithmic or maybe even moving towards a steady limit. The inflation-adjusted values in blue are even more emphatic. Communication still seems to be at least polynomial, although the jumps decrease since the early 2000s. Food is on a gradual linear increase, but pretty consistent. Alcohol &amp; Tobacco fell in relative spending terms from 1971, but since the early 2000s has been fairly static.\n",
    "\n",
    "Of course this is far too little data to do any proper analysis, and the approach has been pretty...rough... but this process should show you how you can apply pandas to your research analysis.\n",
    "\n",
    "One last thing - I keep mentioning tidy code and what you saw up there wasn't really it! A two minute run through, provides the much clearer and readable version below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "## FINAL VERSION OF SCRIPT\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def load_expenditure_data():\n",
    "    \"\"\"Get the ONS expenditure data from an Excel file\"\"\"\n",
    "    \n",
    "    # Load the data from a specific table in the XLS\n",
    "    df = pd.read_excel(\n",
    "        'data/expenditure.xls',\n",
    "        sheet_name='Table 1',\n",
    "        na_values=' ',\n",
    "        header=5,\n",
    "        index_col=0,\n",
    "        usecols=(0, 1, 2, 3, 4, 5, 7)\n",
    "    ).dropna()\n",
    "\n",
    "    # Split data into two parts - ratio against 1971 and the final column,\n",
    "    # an absolute value of bill. GBP spent (for 2009)\n",
    "    df.columns = map(str, df.columns)\n",
    "    perc1971 = df.loc[:, '1971': '2009']\n",
    "    todaysCurrency = pd.DataFrame({'2009': df['2009.1']})\n",
    "\n",
    "    # Make the columns in each part subcolumns under two headings\n",
    "    df = pd.concat((perc1971, todaysCurrency), axis=1, keys=(\"%1971\", \"currency\"))\n",
    "    \n",
    "    # Swap the %1971 and currency into sub-rows for each category\n",
    "    # At the moment, all our currency data is empty, except for 2009\n",
    "    df = df.stack(level=0)\n",
    "\n",
    "    # We actually want rows to be years\n",
    "    df = df.transpose()\n",
    "\n",
    "    # Name the column and index for clarity\n",
    "    df.columns.name = \"Category\"\n",
    "    df.index.name = \"Years\"\n",
    "\n",
    "    # Turn the years into something pandas knows are dates\n",
    "    df.index = pd.to_datetime(df.index, format=\"%Y\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_cpi_for_dates(dates):\n",
    "    \"\"\"Use an online service to get historical Consumer Price Index data\"\"\"\n",
    "    \n",
    "    # Grab the CPI data from Quandl\n",
    "    #cpi = quandl.get(\"UIFS/CPI_GBR\", returns=\"pandas\")\n",
    "    cpi_series = cpi[\"CPI: ALL ITEMS (INDEX NUMBER)\"]\n",
    "\n",
    "    # Create a Series of NaNs, where the index is the dates we want\n",
    "    our_cpi_values = pd.Series(index=dates)\n",
    "    \n",
    "    # Merge our desired dates into the list we have been given\n",
    "    our_cpi_values = pd.concat((cpi_series, our_cpi_values)).sort_index()\n",
    "    \n",
    "    # Interpolate the online CPI values by date, to get rid of the\n",
    "    # NaNs we merged in\n",
    "    our_cpi_values = our_cpi_values.interpolate()\n",
    "    \n",
    "    # Extract only the dates we want\n",
    "    our_cpi_values = our_cpi_values.reindex(dates)\n",
    "    \n",
    "    return our_cpi_values\n",
    "\n",
    "\n",
    "# For comparisons, we we will need 2009 as a DateTime object\n",
    "date2009 = pd.to_datetime(\"2009-01-01 00:00:00\")\n",
    "def calculate_prices_in_old_money_for_col(x):\n",
    "    \"\"\"Fill in the blank (NaN) entries in a column of currency with the non-inflation-adjusted expenditure\"\"\"\n",
    "    # We only need to alter subcolumns with currency\n",
    "    if x.name[1] != 'currency':\n",
    "        return x\n",
    "    \n",
    "    category = x.name[0]\n",
    "    \n",
    "    # This is the multi-index for the %1971 value under the\n",
    "    # same category as column x\n",
    "    percent_col = (category, '%1971')\n",
    "    \n",
    "    # Get the %1971 values in the category for each year\n",
    "    ratio = df.loc[:, percent_col]\n",
    "    \n",
    "    # Get the %1971 value for 2009\n",
    "    ratio2009 = df.loc[date2009, percent_col]\n",
    "    \n",
    "    # Get the CPI values in each year\n",
    "    cpi = our_cpi_values\n",
    "    \n",
    "    # Get the 2009 CPI value\n",
    "    cpi2009 = our_cpi_values[date2009]\n",
    "    \n",
    "    # The currency value is scaled by the CPI difference\n",
    "    # from 2009 and the change in ratio from 2009\n",
    "    x = ratio * x[date2009] * cpi / (ratio2009 * cpi2009)\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "# You only need this line in Jupyter\n",
    "%matplotlib inline\n",
    "def plot_categories(df, categories_of_interest):\n",
    "    \"\"\"\n",
    "    We plot out a series of figures showing the currency and %1971\n",
    "    values for a series of expenditure categories\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set a sensible size of figure\n",
    "    fig = plt.figure(figsize=(20, 5))\n",
    "    number_of_plots = len(categories_of_interest)\n",
    "\n",
    "    # Go through each category desired and create a new subplot\n",
    "    for i, category_name in enumerate(categories_of_interest):\n",
    "        # We create the figure as the (i+1)th place in a (1 x number_of_plots) grid\n",
    "        ax_left = fig.add_subplot(1, number_of_plots, i + 1)\n",
    "        \n",
    "        # We want another ax sharing the same x-axis\n",
    "        ax_right = ax_left.twinx()\n",
    "\n",
    "        # Get the category object from the DataFrame\n",
    "        category = df[category_name]\n",
    "        \n",
    "        # Plot the %1971 bar using LHS\n",
    "        category[\"%1971\"].plot(kind='bar', ax=ax_left, color='blue', label=\"% of 1971\")\n",
    "        \n",
    "        # Plot the currency using RHS y-axis\n",
    "        category[\"currency\"].plot(kind='bar', ax=ax_right, color='red', label=\"Currency\")\n",
    "\n",
    "        plt.title(\"Household Expenditure on \" + category_name)\n",
    "        \n",
    "        # Put the axes legends in the upper left (see docs for loc values)\n",
    "        ax_left.legend(loc=2)\n",
    "        ax_left.set_ylabel(\"% of 1971 (inflation-adjusted)\")\n",
    "        ax_right.legend(loc=9)\n",
    "        ax_right.set_ylabel(\"£ bill. (direct comparison)\")\n",
    "\n",
    "        # Convert the DateTime objects to something pretty\n",
    "        ax_left.set_xticklabels(df.index.map(lambda t: t.strftime(\"%Y\")))\n",
    "        ax_left.set_xlabel(\"Year\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Get our data in\n",
    "df = load_expenditure_data()\n",
    "\n",
    "# Use the Consumer Price Index to work out what the non-inflation-adjusted prices were\n",
    "our_cpi_values = get_cpi_for_dates(df.index)\n",
    "\n",
    "# Update the DataFrame by scaling for CPI\n",
    "df = df.apply(calculate_prices_in_old_money_for_col)\n",
    "\n",
    "# Pick a few categories we want plotted\n",
    "categories_of_interest = (\n",
    "    \"Communication\",\n",
    "    \"Food and non-alcoholic beverages\",\n",
    "    \"Alcoholic beverages and tobacco\"\n",
    ")\n",
    "\n",
    "# Display the grid of plots\n",
    "plot_categories(df, categories_of_interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Well, that's not _quite_ the final version of the script. We wouldn't want to leave you thinking all your pretty business intelligence (BI) tools with their interactive data dashboards are way flasher than what Python has (things like PowerBI and Tableau, which you may or may not have come across).\n",
    "\n",
    "This is departing into a whole separate realm of interactive web-based plotting, so we will simply show how a few changes to use the plotting library, _Bokeh_ (pronounced \"bouquet\", as in \"Hyacinth\" for those old enough to get the reference)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Final Exercise\n",
    "\n",
    "The version below has (largely) the same comments and code as the _matplotlib_ version above. However, it instead uses _bokeh_ to create interactive plots.\n",
    "\n",
    "Suggest corrections, in Etherpad, for the comments that no longer make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "## FINAL VERSION OF SCRIPT, VERSION 2\n",
    "\n",
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "import bokeh.plotting as plt\n",
    "\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.layouts import row\n",
    "from bokeh.models import Range1d, LinearAxis\n",
    "output_notebook()\n",
    "\n",
    "def load_expenditure_data():\n",
    "    \"\"\"Get the ONS expenditure data from an Excel file\"\"\"\n",
    "    \n",
    "    # Load the data from a specific table in the XLS\n",
    "    df = pd.read_excel(\n",
    "        'data/expenditure.xls',\n",
    "        sheet_name='Table 1',\n",
    "        na_values=' ',\n",
    "        header=5,\n",
    "        index_col=0,\n",
    "        usecols=(0, 1, 2, 3, 4, 5, 7)\n",
    "    ).dropna()\n",
    "\n",
    "    # Split data into two parts - ratio against 1971 and the final column,\n",
    "    # an absolute value of bill. GBP spent (for 2009)\n",
    "    df.columns = map(str, df.columns)\n",
    "    perc1971 = df.loc[:, '1971': '2009']\n",
    "    todaysCurrency = pd.DataFrame({'2009': df['2009.1']})\n",
    "\n",
    "    # Make the columns in each part subcolumns under two headings\n",
    "    df = pd.concat((perc1971, todaysCurrency), axis=1, keys=(\"%1971\", \"currency\"))\n",
    "    \n",
    "    # Swap the %1971 and currency into sub-rows for each category\n",
    "    # At the moment, all our currency data is empty, except for 2009\n",
    "    df = df.stack(level=0)\n",
    "\n",
    "    # We actually want rows to be years\n",
    "    df = df.transpose()\n",
    "\n",
    "    # Name the column and index for clarity\n",
    "    df.columns.name = \"Category\"\n",
    "    df.index.name = \"Years\"\n",
    "\n",
    "    # Turn the years into something pandas knows are dates\n",
    "    df.index = pd.to_datetime(df.index, format=\"%Y\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_cpi_for_dates(dates):\n",
    "    \"\"\"Use an online service to get historical Consumer Price Index data\"\"\"\n",
    "    \n",
    "    # Grab the CPI data from Quandl\n",
    "    #cpi = quandl.get(\"UIFS/CPI_GBR\", returns=\"pandas\")\n",
    "    cpi_series = cpi[\"CPI: ALL ITEMS (INDEX NUMBER)\"]\n",
    "\n",
    "    # Create a Series of NaNs, where the index is the dates we want\n",
    "    our_cpi_values = pd.Series(index=dates)\n",
    "    \n",
    "    # Merge our desired dates into the list we have been given\n",
    "    our_cpi_values = pd.concat((cpi_series, our_cpi_values)).sort_index()\n",
    "    \n",
    "    # Interpolate the online CPI values by date, to get rid of the\n",
    "    # NaNs we merged in\n",
    "    our_cpi_values = our_cpi_values.interpolate()\n",
    "    \n",
    "    # Extract only the dates we want\n",
    "    our_cpi_values = our_cpi_values.reindex(dates)\n",
    "    \n",
    "    return our_cpi_values\n",
    "\n",
    "\n",
    "# For comparisons, we we will need 2009 as a DateTime object\n",
    "date2009 = pd.to_datetime(\"2009-01-01 00:00:00\")\n",
    "def calculate_prices_in_old_money_for_col(x):\n",
    "    \"\"\"Fill in the blank (NaN) entries in a column of currency with the non-inflation-adjusted expenditure\"\"\"\n",
    "    # We only need to alter subcolumns with currency\n",
    "    if x.name[1] != 'currency':\n",
    "        return x\n",
    "    \n",
    "    category = x.name[0]\n",
    "    \n",
    "    # This is the multi-index for the %1971 value under the\n",
    "    # same category as column x\n",
    "    percent_col = (category, '%1971')\n",
    "    \n",
    "    # Get the %1971 values in the category for each year\n",
    "    ratio = df.loc[:, percent_col]\n",
    "    \n",
    "    # Get the %1971 value for 2009\n",
    "    ratio2009 = df.loc[date2009, percent_col]\n",
    "    \n",
    "    # Get the CPI values in each year\n",
    "    cpi = our_cpi_values\n",
    "    \n",
    "    # Get the 2009 CPI value\n",
    "    cpi2009 = our_cpi_values[date2009]\n",
    "    \n",
    "    # The currency value is scaled by the CPI difference\n",
    "    # from 2009 and the change in ratio from 2009\n",
    "    x = ratio * x[date2009] * cpi / (ratio2009 * cpi2009)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def plot_categories(df, categories_of_interest):\n",
    "    \"\"\"\n",
    "    We plot out a series of figures showing the currency and %1971\n",
    "    values for a series of expenditure categories\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set a sensible size of figure\n",
    "    \n",
    "    number_of_plots = len(categories_of_interest)\n",
    "    plots = []\n",
    "    \n",
    "    # Go through each category desired and create a new subplot\n",
    "    for i, category_name in enumerate(categories_of_interest):\n",
    "        # We create the figure as the (i+1)th place in a (1 x number_of_plots) grid\n",
    "        ax_left = plt.figure()\n",
    "        fig = plt.figure(\n",
    "            title = category_name,\n",
    "            x_axis_type='datetime',\n",
    "            plot_width=300,\n",
    "            plot_height=500,\n",
    "            toolbar_location='below',\n",
    "            active_scroll='wheel_zoom',\n",
    "            tools='pan,wheel_zoom,box_zoom,save,reset'\n",
    "        )\n",
    "        \n",
    "        # Get the category object from the DataFrame\n",
    "        category = df[category_name]\n",
    "                \n",
    "        # We want another ax sharing the same x-axis\n",
    "        fig.y_range = Range1d(start=0, end=category[\"%1971\"].max() * 1.1)\n",
    "        fig.extra_y_ranges = {'currency': Range1d(start=0, end=100)}\n",
    "\n",
    "        # Plot the %1971 bar using LHS\n",
    "        fig.vbar(\n",
    "            x=category.index,\n",
    "            top=category[\"%1971\"],\n",
    "            width=2e11,\n",
    "            legend_label=\"% of 1971\"\n",
    "        )\n",
    "        \n",
    "        # Plot the currency using RHS y-axis\n",
    "        fig.vbar(\n",
    "            x=category.index,\n",
    "            top=category[\"currency\"],\n",
    "            width=2e11,\n",
    "            color='red',\n",
    "            y_range_name='currency',\n",
    "            legend_label=\"Currency\"\n",
    "        )\n",
    "        \n",
    "        # Put the axes legends in the upper left (see docs for loc values)\n",
    "        fig.legend.location = 'top_left'\n",
    "        fig.yaxis.axis_label = \"% of 1971 (inflation-adjusted)\"\n",
    "        fig.add_layout(LinearAxis(y_range_name='currency', axis_label=\"£ bill. (direct comparison)\"), 'right')\n",
    "\n",
    "        # Convert the DateTime objects to something pretty\n",
    "        fig.xaxis.axis_label = \"Year\"\n",
    "        \n",
    "        plots.append(fig)\n",
    "\n",
    "    plt.show(row(*plots))\n",
    "\n",
    "\n",
    "# Get our data in\n",
    "df = load_expenditure_data()\n",
    "\n",
    "# Use the Consumer Price Index to work out what the non-inflation-adjusted prices were\n",
    "our_cpi_values = get_cpi_for_dates(df.index)\n",
    "\n",
    "# Update the DataFrame by scaling for CPI\n",
    "df = df.apply(calculate_prices_in_old_money_for_col)\n",
    "\n",
    "# Pick a few categories we want plotted\n",
    "categories_of_interest = (\n",
    "    \"Communication\",\n",
    "    \"Food and non-alcoholic beverages\",\n",
    "    \"Alcoholic beverages and tobacco\"\n",
    ")\n",
    "\n",
    "# Display the grid of plots\n",
    "plot_categories(df, categories_of_interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "_Bokeh_ also allows you have interactive buttons and sliders, as well as many of the plotting types available in _matplotlib_. You could then export this as an interactive page that you can embed in a website, for instance.\n",
    "\n",
    "In the most recent versions of _bokeh_, you can in a single short line make the legend clickable to hide individual plots: `fig.legend.click_policy = 'hide'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "If there is some time left, or you are coming back to this as an exercise, try using pandas to export an Excel file, or if you are feeling brave, export to SQL and create an Access database with [PyPyODBC](https://code.google.com/archive/p/pypyodbc/wikis/PyPyODBC_Example_Tutorial.wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "A side note on big data: some tasks like MapReduce, and other buzzwordy things engineers like me don't deal with much, have specialized non-Python tools you may want to use, but Python is the best tool for analysing the output and passing to something else. Maybe you just have a series of bash scripts, or another tool for generating data. If this sounds like a use-case for you, you may wish to check out [luigi](https://github.com/spotify/luigi), an open source Python-based pipeline management tool from Spotify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Another tip if you need your Python fast - you can use Cython to translate it from Python to C and compile it automatically. This is common in science and data analysis for accelerating your Python code. If you're proficient in C then, when you start getting that way in Python, you could check out something like Philip Herron's book *Learning Cython* - he is a Belfast-based developer who has used these tools working for the New York Stock Exchange."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "livereveal": {
   "theme": "beige",
   "transition": "convex"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
