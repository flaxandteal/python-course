{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Words Apart\n",
    "## Levenshtein Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How far do you say?\n",
    "### Measuring the gaps between words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bioinformatics studies the building blocks of life. To examine a few more Python tools, we will look at the most basic building block of bioinformatics - [edit distance](https://en.wikipedia.org/wiki/Edit_distance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how you can measure the separation between two strings of letters. If you remember the sci-fi film Gattica (about a world gone genetic-testing crazy), or even school biology, you may be aware that our genes are made up of long DNA sequences of nucleic acids, called _bases_ . There are only four bases: `G`, `A`, `T` and `C`. Guess where the film name comes from..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means you can understand a lot about genetic similarity by comparing long sequences and working our how far apart they are - perhaps what genetic mutations would get from one version to another, to understand what animals, plants or bacteria are related and why they differ.\n",
    "\n",
    "GGACTATCTACTACCATACGGACTATCTACTACCATACGGACTATCTACTACCATACGGACTATCTACTACCATACGGACTATCTACTACCATAC...\n",
    "GAGCTATCTACCTAGCATTCGACTAACTACTACCATTCGGACTATCTACTACCATACGGACTATCTACTACCATACGGACTATCTACTACCATAC..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see these are similar, but not quite the same, but more more alike than either are to:\n",
    "GGGGGGGGGGGGGGGGGGGAAAAAAAAAAAAAAAAAAAAAAAAAATTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTCCCCCCCCCCCCC..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we quantify that? One way:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Levenshtein Distance**: (roughly) the minimum number of single-character edits to get from the first string to the second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this exercise, we will touch on some of the tools that will become more foundational in the IDE setting, with VSCode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip` is one of a wide range of tools constituting the Python packaging ecosystem. It is hugely fragmented compared to most languages, but `pip` is a relatively simple and standard way of installing tools. You may also come across the Anaconda distribution, and its tool `conda`, which works very similarly. We need additional libraries for this exercise - you can install them as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-Levenshtein\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/a9/d1785c85ebf9b7dfacd08938dd028209c34a0ea3b1bcdb895208bd40a67d/python-Levenshtein-0.12.0.tar.gz (48kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 10.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytest-benchmark\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/1e/180579ad3bc53fe3181ef3843f0602f4db77f3609e5e5069a0ec194ff213/pytest_benchmark-3.2.3-py2.py3-none-any.whl (49kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 14.2MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from python-Levenshtein) (41.0.1)\n",
      "Collecting pytest>=3.8 (from pytest-benchmark)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/f3/0a83558da436a081344aa6c8b85ea5b5f05071214106036ce341b7769b0b/pytest-5.4.3-py3-none-any.whl (248kB)\n",
      "\u001b[K     |████████████████████████████████| 256kB 20.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting py-cpuinfo (from pytest-benchmark)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/60/63f28a5401da733043abe7053e7d9591491b4784c4f87c339bf51215aa0a/py-cpuinfo-5.0.0.tar.gz (82kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 31.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from pytest>=3.8->pytest-benchmark) (0.1.7)\n",
      "Collecting py>=1.5.0 (from pytest>=3.8->pytest-benchmark)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/8d/21e1767c009211a62a8e3067280bfce76e89c9f876180308515942304d2d/py-1.8.1-py2.py3-none-any.whl (83kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 22.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from pytest>=3.8->pytest-benchmark) (19.1.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from pytest>=3.8->pytest-benchmark) (19.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from pytest>=3.8->pytest-benchmark) (8.3.0)\n",
      "Collecting pluggy<1.0,>=0.12 (from pytest>=3.8->pytest-benchmark)\n",
      "  Downloading https://files.pythonhosted.org/packages/a0/28/85c7aa31b80d150b772fbe4a229487bc6644da9ccb7e427dd8cc60cb8a62/pluggy-0.13.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from pytest>=3.8->pytest-benchmark) (1.6.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging->pytest>=3.8->pytest-benchmark) (1.12.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->pytest>=3.8->pytest-benchmark) (2.4.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=3.8->pytest-benchmark) (3.1.0)\n",
      "Building wheels for collected packages: python-Levenshtein, py-cpuinfo\n",
      "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.0-cp37-cp37m-linux_x86_64.whl size=167825 sha256=58a25d592edec836b9a5d01f0e91216c4d649d31f94b0610f2f7d977d243b485\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/de/c2/93/660fd5f7559049268ad2dc6d81c4e39e9e36518766eaf7e342\n",
      "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for py-cpuinfo: filename=py_cpuinfo-5.0.0-cp37-none-any.whl size=18683 sha256=b03e9061c7b8ed4668e4e4e4e6195c6ef139c2dcfcdf974598a50b8a7f212fe4\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/01/7e/a9/b982d0fea22b7e4ae5619de949570cde5ad55420cec16e86a5\n",
      "Successfully built python-Levenshtein py-cpuinfo\n",
      "Installing collected packages: python-Levenshtein, py, pluggy, pytest, py-cpuinfo, pytest-benchmark\n",
      "Successfully installed pluggy-0.13.1 py-1.8.1 py-cpuinfo-5.0.0 pytest-5.4.3 pytest-benchmark-3.2.3 python-Levenshtein-0.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install python-Levenshtein pytest-benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good practice is to have a requirements file, where version limits can be set for each package - to avoid accidental breaking upgrades (a common standard is to pin the major version number, as under semantic versioning practice, minor versions should not introduce major breaking changes). Then, when you clone down the repository, one way of installing dependencies is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (1.17.0)\n",
      "Requirement already satisfied: pytest in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (5.4.3)\n",
      "Requirement already satisfied: pytest-benchmark==3.2.* in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (3.2.3)\n",
      "Requirement already satisfied: python-levenshtein<0.13,>=0.10 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (0.12.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from pytest->-r requirements.txt (line 2)) (0.1.7)\n",
      "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from pytest->-r requirements.txt (line 2)) (1.6.0)\n",
      "Requirement already satisfied: pluggy<1.0,>=0.12 in /opt/conda/lib/python3.7/site-packages (from pytest->-r requirements.txt (line 2)) (0.13.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from pytest->-r requirements.txt (line 2)) (19.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from pytest->-r requirements.txt (line 2)) (8.3.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from pytest->-r requirements.txt (line 2)) (19.1.0)\n",
      "Requirement already satisfied: py>=1.5.0 in /opt/conda/lib/python3.7/site-packages (from pytest->-r requirements.txt (line 2)) (1.8.1)\n",
      "Requirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.7/site-packages (from pytest-benchmark==3.2.*->-r requirements.txt (line 3)) (5.0.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from python-levenshtein<0.13,>=0.10->-r requirements.txt (line 4)) (41.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest->-r requirements.txt (line 2)) (3.1.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging->pytest->-r requirements.txt (line 2)) (1.12.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->pytest->-r requirements.txt (line 2)) (2.4.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy\r\n",
      "pytest\r\n",
      "pytest-benchmark ==3.2.*\r\n",
      "python-levenshtein >=0.10, <0.13\r\n"
     ]
    }
   ],
   "source": [
    "!cat requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that `pytest-benchmark` and `python-levenshtein` are pinned - ideally being generous down the way, but not giving _too_ much flexibility up the way (you don't know if a breaking change will come in a dependency's new version), will help ensure your dependencies can be met but the risk of third-party breakages is reduced. In production deployment, exact pinning and dedicated repositories are strongly recommended - some language tools do this in a more streamlined way (package.lock for npm, for example). However... narrowly pinning development code can prevent security patches being brought in, or conflicts when your module is later used with other code - the dependency versions conflict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first contact with `pytest`. It automatically seeks out files starting with `test_` -- I'll walk you through `test_levenshtein.py` now..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run these tests from the command line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.3, pytest-5.4.2, py-1.8.1, pluggy-0.13.1\n",
      "benchmark: 3.2.3 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\n",
      "rootdir: /home/jovyan/python-course/018-bioinformatics\n",
      "plugins: benchmark-3.2.3\n",
      "collected 5 items                                                              \u001b[0m\n",
      "\n",
      "test_levenshtein.py \u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_________________ test_different_strings_have_correct_distance _________________\u001b[0m\n",
      "\n",
      "string1 = 'ttcagttcctcgcctatccaaacagcagcgtgaggtgcaagggacaaaccgatagggttgacacgttatgactatctgagtcttcggggtacgaacgagttgaggtaattgacatgg...gttggcggtacgtacggtgcagcagtgacgcaggaccagattgagttactacgaagaaacctgcaggcagcctggctgttcccgtagtgtatccgtacggttcgggcttgggcagcaa'\n",
      "string2 = 'ttcagttcctcgcctatccaaacagcagcgtgaggtgcaagggacaaaccgatagggttgacacgttatgactatctgagtcttcggggtacgaacgagttgaggtaattgacatgg...gttggcggtacgtacggtgcagcagtgacgcaggaccagattgagttactacgaagaaacctgcaggcagcctggctgttcccgtagtgtatccgtacggttcgggcttgggcagcaa'\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_different_strings_have_correct_distance\u001b[39;49;00m(string1, string2):\n",
      "        edit_distance = my_levenshtein.calculate_levenshtein(string1, string2)\n",
      ">       \u001b[94massert\u001b[39;49;00m edit_distance == levenshtein_module.distance(string1, string2)\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert 0 == 200\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 200 = <built-in function distance>('ttcagttcctcgcctatccaaacagcagcgtgaggtgcaagggacaaaccgatagggttgacacgttatgactatctgagtcttcggggtacgaacgagttgaggtaattgacatgg...gttggcggtacgtacggtgcagcagtgacgcaggaccagattgagttactacgaagaaacctgcaggcagcctggctgttcccgtagtgtatccgtacggttcgggcttgggcagcaa', 'ttcagttcctcgcctatccaaacagcagcgtgaggtgcaagggacaaaccgatagggttgacacgttatgactatctgagtcttcggggtacgaacgagttgaggtaattgacatgg...gttggcggtacgtacggtgcagcagtgacgcaggaccagattgagttactacgaagaaacctgcaggcagcctggctgttcccgtagtgtatccgtacggttcgggcttgggcagcaa')\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +    where <built-in function distance> = levenshtein_module.distance\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_levenshtein.py\u001b[0m:41: AssertionError\n",
      "\u001b[31m\u001b[1m________________________ test_asdf_asfd_distance_is_two ________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_asdf_asfd_distance_is_two\u001b[39;49;00m():\n",
      "        edit_distance = my_levenshtein.calculate_levenshtein(\u001b[33m\"\u001b[39;49;00m\u001b[33masdf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33masfd\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      ">       \u001b[94massert\u001b[39;49;00m edit_distance == \u001b[94m2\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert 0 == 2\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_levenshtein.py\u001b[0m:45: AssertionError\n",
      "\u001b[31m\u001b[1m________________________ test_asf_asfd_distance_is_one _________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_asf_asfd_distance_is_one\u001b[39;49;00m():\n",
      "        edit_distance = my_levenshtein.calculate_levenshtein(\u001b[33m\"\u001b[39;49;00m\u001b[33masf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33masfd\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      ">       \u001b[94massert\u001b[39;49;00m edit_distance == \u001b[94m1\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert 0 == 1\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_levenshtein.py\u001b[0m:49: AssertionError\n",
      "\u001b[31m\u001b[1m____________________ test_amazon_aazonia_distance_is_three _____________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_amazon_aazonia_distance_is_three\u001b[39;49;00m():\n",
      "        edit_distance = my_levenshtein.calculate_levenshtein(\u001b[33m\"\u001b[39;49;00m\u001b[33mamazon\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33maazonia\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      ">       \u001b[94massert\u001b[39;49;00m edit_distance == \u001b[94m3\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert 0 == 3\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_levenshtein.py\u001b[0m:53: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED test_levenshtein.py::test_different_strings_have_correct_distance - As...\n",
      "FAILED test_levenshtein.py::test_asdf_asfd_distance_is_two - assert 0 == 2\n",
      "FAILED test_levenshtein.py::test_asf_asfd_distance_is_one - assert 0 == 1\n",
      "FAILED test_levenshtein.py::test_amazon_aazonia_distance_is_three - assert 0 ...\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m4 failed\u001b[0m, \u001b[32m1 passed\u001b[0m\u001b[31m in 5.90s\u001b[0m\u001b[31m ==========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest test_levenshtein.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Gene Hacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a link there to Wikipedia, which has a standard textbook description of two algorithms - we will test which is faster. Can you fill in `my_levenshtein.py` to implement the _recursive_ `calculate_levenshtein` algorithm? Re-run the `pytest` command above until it passes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension: Gene Wilder\n",
    "\n",
    "Going one further, can you implement the matrix version, using numpy, in the routine below? (algorithm also available from the same source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension: Gene E Us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you write a version that passes all the tests, but does not work in general? Can you add a test to catch your \"mistake\"? How much more robust can you make the testing?\n",
    "\n",
    "This highlights a challenge with testing numerical or ML algorithms - that enumerating all possible cases is not necessarily possible. How might you break it up your testing, or your algorithm, to be able to more reliably test it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pylint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pylint exists to help make sure your code is compliant with the PEP8 style guide (and a few others) - you can run it like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* Module my_levenshtein\n",
      "my_levenshtein.py:9:0: E0401: Unable to import 'python_course_levenshtein_py' (import-error)\n",
      "my_levenshtein.py:12:0: C0103: Argument name \"a\" doesn't conform to snake_case naming style (invalid-name)\n",
      "my_levenshtein.py:12:0: C0103: Argument name \"b\" doesn't conform to snake_case naming style (invalid-name)\n",
      "my_levenshtein.py:12:0: C0116: Missing function or method docstring (missing-function-docstring)\n",
      "my_levenshtein.py:12:26: W0613: Unused argument 'a' (unused-argument)\n",
      "my_levenshtein.py:12:29: W0613: Unused argument 'b' (unused-argument)\n",
      "my_levenshtein.py:20:0: C0103: Argument name \"a\" doesn't conform to snake_case naming style (invalid-name)\n",
      "my_levenshtein.py:20:0: C0103: Argument name \"b\" doesn't conform to snake_case naming style (invalid-name)\n",
      "my_levenshtein.py:20:0: C0116: Missing function or method docstring (missing-function-docstring)\n",
      "my_levenshtein.py:22:4: C0103: Variable name \"l\" doesn't conform to snake_case naming style (invalid-name)\n",
      "\n",
      "------------------------------------------------------------------\n",
      "Your code has been rated at 0.00/10 (previous run: 0.00/10, +0.00)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pylint my_levenshtein.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a pain, but once your code passes it, it's a breeze! It is recommended to include it in a continuous integration pipeline, just like testing - some developers include it in the githooks (automated code that is run when code is either committed locally, or pushed remotely)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Code Hy-gene\n",
    "\n",
    "Try to adapt your code until it passes. Are all the checks useful? Are there some you would switch off?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "livereveal": {
   "theme": "beige",
   "transition": "cascade"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
